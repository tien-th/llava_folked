[2025-03-02 10:20:21,312] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/user01/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-02 10:20:22,973] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0: setting --include=localhost:0
[2025-03-02 10:20:22,973] [INFO] [runner.py:607:main] cmd = /home/user01/miniconda3/envs/llava_thaind/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --lora_enable True --model_name_or_path ./checkpoints/llava-med-v1.5-mistral-7b --version v1 --data_path /home/user01/aiotlab/thaind/data_desc_conv_train.json --eval_data_path /home/user01/aiotlab/thaind/data_desc_conv_eval.json --image_folder /home/user01/aiotlab/thaind/DAC001_CTAC3.75mm_H_1001_PETWB3DAC001 --vision_tower /home/user01/aiotlab/htien/pet-clip/ViT_ckpts/CTVit.39000.pt --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints/ctvit_llavamed-llava-med-v1.5-mistral-7b-finetune_lora_1epochs --num_train_epochs 3 --per_device_train_batch_size 16 --per_device_eval_batch_size 8 --gradient_accumulation_steps 1 --eval_strategy epoch --save_strategy epoch --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --lazy_preprocess True --dataloader_num_workers 2 --report_to wandb
[2025-03-02 10:20:25,426] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/user01/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-02 10:20:26,848] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-03-02 10:20:26,848] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-03-02 10:20:26,848] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-03-02 10:20:26,848] [INFO] [launch.py:164:main] dist_world_size=1
[2025-03-02 10:20:26,848] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-03-02 10:20:26,871] [INFO] [launch.py:256:main] process 3365547 spawned with command: ['/home/user01/miniconda3/envs/llava_thaind/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero2.json', '--lora_enable', 'True', '--model_name_or_path', './checkpoints/llava-med-v1.5-mistral-7b', '--version', 'v1', '--data_path', '/home/user01/aiotlab/thaind/data_desc_conv_train.json', '--eval_data_path', '/home/user01/aiotlab/thaind/data_desc_conv_eval.json', '--image_folder', '/home/user01/aiotlab/thaind/DAC001_CTAC3.75mm_H_1001_PETWB3DAC001', '--vision_tower', '/home/user01/aiotlab/htien/pet-clip/ViT_ckpts/CTVit.39000.pt', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoints/ctvit_llavamed-llava-med-v1.5-mistral-7b-finetune_lora_1epochs', '--num_train_epochs', '3', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '8', '--gradient_accumulation_steps', '1', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--lazy_preprocess', 'True', '--dataloader_num_workers', '2', '--report_to', 'wandb']
[2025-03-02 10:20:33,201] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/user01/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-02 10:20:34,039] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-03-02 10:20:34,039] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
initialize_vision_modules: build_vision_tower: 35
config:  LlavaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "./checkpoints/llava-med-v1.5-mistral-7b",
  "architectures": [
    "LlavaMistralForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "feature_outs": "encoder+decoder",
  "freeze_mm_mlp_adapter": false,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "image_aspect_ratio": "pad",
  "img_size": 640,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "mlp_bias": false,
  "mm_hidden_size": 1024,
  "mm_projector_lr": null,
  "mm_projector_type": "mlp2x_gelu",
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14-336",
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "proj_vis_to_txt_tokens": false,
  "prompt_segtok_w_instruct": false,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "segtok_posembed": "sincos",
  "sliding_window": null,
  "tie_word_embeddings": false,
  "tokenizer_model_max_length": 2048,
  "tokenizer_padding_side": "right",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "tune_mm_mlp_adapter": false,
  "tune_vision_tokenizer": "none",
  "use_cache": true,
  "use_mm_proj": true,
  "vision_backbone": "convnextlarge",
  "vision_tokenizer_lr": null,
  "vocab_size": 32000
}

mlp2x_gelu 1024 4096
Adding LoRA adapters...
initialize_vision_modules
get_vision_tower is None 61
Loading Vision Tower from:  /home/user01/aiotlab/htien/pet-clip/ViT_ckpts/CTVit.39000.pt
Using mm_hidden_size: 294912
initialize_vision_modules: build_vision_projector: 81
linear 294912 4096
training_args:  TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
bits=16,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./scripts/zero2.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
double_quant=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=epoch,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
freeze_mm_mlp_adapter=False,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
group_by_modality_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/ctvit_llavamed-llava-med-v1.5-mistral-7b-finetune_lora_1epochs/runs/Mar02_10-20-33_dgx01,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lora_alpha=16,
lora_bias=none,
lora_dropout=0.05,
lora_enable=True,
lora_r=64,
lora_weight_path=,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mm_projector_lr=None,
model_max_length=2048,
mp_parameters=,
mpt_attn_impl=triton,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=./checkpoints/ctvit_llavamed-llava-med-v1.5-mistral-7b-finetune_lora_1epochs,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
quant_type=nf4,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=./checkpoints/ctvit_llavamed-llava-med-v1.5-mistral-7b-finetune_lora_1epochs,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.7781, 'grad_norm': 1.1780633926391602, 'learning_rate': 1.975654797438384e-05, 'epoch': 0.3}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.6554, 'grad_norm': 1.0646586418151855, 'learning_rate': 1.9751470863654773e-05, 'epoch': 0.3}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.6929, 'grad_norm': 0.8663181662559509, 'learning_rate': 1.974634202310892e-05, 'epoch': 0.3}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.7882, 'grad_norm': 0.8998779058456421, 'learning_rate': 1.9741161479953872e-05, 'epoch': 0.3}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.5096, 'grad_norm': 0.6413795948028564, 'learning_rate': 1.9735929261671484e-05, 'epoch': 0.3}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.6091, 'grad_norm': 0.5470871925354004, 'learning_rate': 1.9730645396017746e-05, 'epoch': 0.31}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.6127, 'grad_norm': 0.5148863196372986, 'learning_rate': 1.9725309911022617e-05, 'epoch': 0.31}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.5754, 'grad_norm': 0.5489664673805237, 'learning_rate': 1.9719922834989905e-05, 'epoch': 0.31}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.566, 'grad_norm': 0.5059677362442017, 'learning_rate': 1.9714484196497087e-05, 'epoch': 0.31}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.556, 'grad_norm': 0.48155319690704346, 'learning_rate': 1.9708994024395163e-05, 'epoch': 0.32}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.5684, 'grad_norm': 0.45129430294036865, 'learning_rate': 1.9703452347808527e-05, 'epoch': 0.32}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.5862, 'grad_norm': 0.4779747426509857, 'learning_rate': 1.9697859196134786e-05, 'epoch': 0.32}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.4303, 'grad_norm': 0.5725398659706116, 'learning_rate': 1.969221459904461e-05, 'epoch': 0.32}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.5261, 'grad_norm': 0.4433760643005371, 'learning_rate': 1.9686518586481585e-05, 'epoch': 0.32}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.5494, 'grad_norm': 0.4213232100009918, 'learning_rate': 1.9680771188662044e-05, 'epoch': 0.33}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.5041, 'grad_norm': 0.43358662724494934, 'learning_rate': 1.9674972436074907e-05, 'epoch': 0.33}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.434, 'grad_norm': 0.4552384614944458, 'learning_rate': 1.9669122359481526e-05, 'epoch': 0.33}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.4924, 'grad_norm': 0.4658227562904358, 'learning_rate': 1.9663220989915513e-05, 'epoch': 0.33}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.3401, 'grad_norm': 0.590568482875824, 'learning_rate': 1.9657268358682584e-05, 'epoch': 0.33}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.4303, 'grad_norm': 0.43709874153137207, 'learning_rate': 1.965126449736039e-05, 'epoch': 0.34}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.3779, 'grad_norm': 0.45459651947021484, 'learning_rate': 1.964520943779834e-05, 'epoch': 0.34}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.4661, 'grad_norm': 0.4247881770133972, 'learning_rate': 1.9639103212117458e-05, 'epoch': 0.34}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.3791, 'grad_norm': 0.4580361545085907, 'learning_rate': 1.9632945852710175e-05, 'epoch': 0.34}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.436, 'grad_norm': 0.41032281517982483, 'learning_rate': 1.962673739224019e-05, 'epoch': 0.35}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.4596, 'grad_norm': 0.4844622015953064, 'learning_rate': 1.9620477863642277e-05, 'epoch': 0.35}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.392, 'grad_norm': 0.3808039128780365, 'learning_rate': 1.9614167300122126e-05, 'epoch': 0.35}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.3527, 'grad_norm': 0.45185282826423645, 'learning_rate': 1.960780573515615e-05, 'epoch': 0.35}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.3232, 'grad_norm': 0.4045603573322296, 'learning_rate': 1.9601393202491316e-05, 'epoch': 0.35}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
{'loss': 0.4198, 'grad_norm': 0.5159357786178589, 'learning_rate': 1.9594929736144978e-05, 'epoch': 0.36}
images:  torch.Size([16, 1, 140, 480, 480])
tokens: torch.bfloat16
torch.Size([16, 294912])
image_features:  torch.Size([16, 4096])
